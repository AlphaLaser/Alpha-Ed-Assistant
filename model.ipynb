{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-f5eb1d4d-820f-4c21-af8b-098e069a50e2",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ae04eb4e",
    "execution_start": 1624722979974,
    "execution_millis": 8887,
    "deepnote_cell_type": "code"
   },
   "source": "import random\nimport json\nimport pickle\nimport numpy as np\n\nimport nltk\nnltk.download('pun')\nfrom nltk.stem import WordNetLemmatizer\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import SGD\n\nlemmatizer = WordNetLemmatizer()\n\nintents = json.loads(open('q.json').read())\n\nwords = []\nclasses = []\ndocuments = []\nignore_letters = ['?','!',',','.','\\'']\n\n\n\nfor intent in intents['intents']:\n    for pattern in intent['patterns']:\n        word_list = nltk.word_tokenize(pattern)\n        words.extend(word_list)\n        documents.append((word_list, intent['tag']))\n        if intent['tag'] not in classes:\n            classes.append(intent['tag'])\n\nwords = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\nwords = sorted(set(words))\n\nclasses = sorted(set(classes))\n\npickle.dump(words, open('words.pkl','wb'))\npickle.dump(classes, open('classes.pkl','wb'))\nprint(len(words))\ntraining = []\noutput_empty = [0] * len(classes)\n\nfor document in documents :\n    bag = []\n    word_patterns = document[0]\n    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns ]\n    for word in words :\n       bag.append(1) if word in word_patterns else bag.append(0)\n\n    output_row = list(output_empty)\n    output_row[classes.index(document[1])] = 1\n    training.append([bag, output_row])\n\nrandom.shuffle(training)\ntraining = np.array(training)\n\nx_train = list(training[:, 0])\ny_train = list(training[:, 1])\n\nmodel = Sequential()\nmodel.add(Dense(256, input_shape=(len(x_train[0]),), activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(len(y_train[0]),activation='softmax'))\n\nsgd = SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n\nhistory = model.fit(np.array(x_train), np.array(y_train), epochs = 170, batch_size=5, verbose = 1 )\n\nmodel.save('chatbot_model.h5', history)\n\nprint(\"Done\")\n\n\n\n\n\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "[nltk_data] Error loading pun: Package 'pun' not found in index\n108\n/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:56: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\nEpoch 1/170\n15/15 [==============================] - 1s 2ms/step - loss: 2.8112 - accuracy: 0.0163 \nEpoch 2/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.7694 - accuracy: 0.0111\nEpoch 3/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.7829 - accuracy: 0.0730\nEpoch 4/170\n15/15 [==============================] - 0s 3ms/step - loss: 2.7463 - accuracy: 0.1203\nEpoch 5/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.6881 - accuracy: 0.2360\nEpoch 6/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.6135 - accuracy: 0.1558\nEpoch 7/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.6753 - accuracy: 0.1223\nEpoch 8/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.5715 - accuracy: 0.1789\nEpoch 9/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.5615 - accuracy: 0.0923\nEpoch 10/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.4808 - accuracy: 0.1594\nEpoch 11/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.4652 - accuracy: 0.2380\nEpoch 12/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.4075 - accuracy: 0.2522\nEpoch 13/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.2728 - accuracy: 0.3435\nEpoch 14/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.5715 - accuracy: 0.1104\nEpoch 15/170\n15/15 [==============================] - 0s 3ms/step - loss: 2.3152 - accuracy: 0.1475\nEpoch 16/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.4722 - accuracy: 0.1593\nEpoch 17/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.3289 - accuracy: 0.2238\nEpoch 18/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.4146 - accuracy: 0.2221\nEpoch 19/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.3601 - accuracy: 0.1273\nEpoch 20/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.4257 - accuracy: 0.1406\nEpoch 21/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.1786 - accuracy: 0.2626\nEpoch 22/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.0055 - accuracy: 0.2798\nEpoch 23/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.0277 - accuracy: 0.2761\nEpoch 24/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.2022 - accuracy: 0.1589\nEpoch 25/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.0143 - accuracy: 0.3013\nEpoch 26/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.8587 - accuracy: 0.3828\nEpoch 27/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.2487 - accuracy: 0.2253\nEpoch 28/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.9944 - accuracy: 0.3535\nEpoch 29/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.3345 - accuracy: 0.1885\nEpoch 30/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.8487 - accuracy: 0.3926\nEpoch 31/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.9784 - accuracy: 0.4146\nEpoch 32/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.9708 - accuracy: 0.2935\nEpoch 33/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.1542 - accuracy: 0.3240\nEpoch 34/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.1458 - accuracy: 0.2745\nEpoch 35/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.2763 - accuracy: 0.2010\nEpoch 36/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.9873 - accuracy: 0.3708\nEpoch 37/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.8421 - accuracy: 0.3769\nEpoch 38/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.6693 - accuracy: 0.5151\nEpoch 39/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.8587 - accuracy: 0.4515\nEpoch 40/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.2739 - accuracy: 0.3228\nEpoch 41/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.2009 - accuracy: 0.2251\nEpoch 42/170\n15/15 [==============================] - 0s 3ms/step - loss: 2.0906 - accuracy: 0.4397\nEpoch 43/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.0580 - accuracy: 0.2859\nEpoch 44/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.8302 - accuracy: 0.3821\nEpoch 45/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.9365 - accuracy: 0.3365\nEpoch 46/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.9523 - accuracy: 0.3519\nEpoch 47/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.8937 - accuracy: 0.3306\nEpoch 48/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.9753 - accuracy: 0.3220\nEpoch 49/170\n15/15 [==============================] - 0s 3ms/step - loss: 1.9075 - accuracy: 0.3168\nEpoch 50/170\n15/15 [==============================] - 0s 3ms/step - loss: 1.6768 - accuracy: 0.3972\nEpoch 51/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.6328 - accuracy: 0.4729\nEpoch 52/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.7825 - accuracy: 0.4032\nEpoch 53/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.9891 - accuracy: 0.4662\nEpoch 54/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.7062 - accuracy: 0.4352\nEpoch 55/170\n15/15 [==============================] - 0s 2ms/step - loss: 3.3633 - accuracy: 0.3643\nEpoch 56/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.9264 - accuracy: 0.3112\nEpoch 57/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.9564 - accuracy: 0.4329\nEpoch 58/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.1174 - accuracy: 0.3061\nEpoch 59/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.7228 - accuracy: 0.3956\nEpoch 60/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.8236 - accuracy: 0.3989\nEpoch 61/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.7539 - accuracy: 0.4703\nEpoch 62/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.5622 - accuracy: 0.4676\nEpoch 63/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.6072 - accuracy: 0.5123\nEpoch 64/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.6901 - accuracy: 0.4461\nEpoch 65/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.0061 - accuracy: 0.4274\nEpoch 66/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.9422 - accuracy: 0.3921\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=50.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nEpoch 112/170\n15/15 [==============================] - 0s 3ms/step - loss: 1.3896 - accuracy: 0.4976\nEpoch 113/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.2976 - accuracy: 0.5761\nEpoch 114/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.4101 - accuracy: 0.4786\nEpoch 115/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.4198 - accuracy: 0.5797\nEpoch 116/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.7243 - accuracy: 0.5067\nEpoch 117/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.7354 - accuracy: 0.4921\nEpoch 118/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.4383 - accuracy: 0.5247\nEpoch 119/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.3675 - accuracy: 0.4944\nEpoch 120/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.4243 - accuracy: 0.4602\nEpoch 121/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.2749 - accuracy: 0.5367\nEpoch 122/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.2591 - accuracy: 0.6638\nEpoch 123/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.5216 - accuracy: 0.4245\nEpoch 124/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.2299 - accuracy: 0.5334\nEpoch 125/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.6979 - accuracy: 0.4945\nEpoch 126/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.4838 - accuracy: 0.4921\nEpoch 127/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.3354 - accuracy: 0.4940\nEpoch 128/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.5599 - accuracy: 0.5482\nEpoch 129/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.0021 - accuracy: 0.5367\nEpoch 130/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.5183 - accuracy: 0.5755\nEpoch 131/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.3377 - accuracy: 0.5667\nEpoch 132/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.9505 - accuracy: 0.4795\nEpoch 133/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.9928 - accuracy: 0.5223\nEpoch 134/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.3598 - accuracy: 0.5702\nEpoch 135/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.4380 - accuracy: 0.4843\nEpoch 136/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.3311 - accuracy: 0.5067\nEpoch 137/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.6453 - accuracy: 0.4883\nEpoch 138/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.5884 - accuracy: 0.4844\nEpoch 139/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.2940 - accuracy: 0.6055\nEpoch 140/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.9692 - accuracy: 0.4597\nEpoch 141/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.4401 - accuracy: 0.5310\nEpoch 142/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.9871 - accuracy: 0.4894\nEpoch 143/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.9874 - accuracy: 0.4283\nEpoch 144/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.1815 - accuracy: 0.4662\nEpoch 145/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.6378 - accuracy: 0.5714\nEpoch 146/170\n15/15 [==============================] - 0s 1ms/step - loss: 2.0058 - accuracy: 0.4044\nEpoch 147/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.7539 - accuracy: 0.4369\nEpoch 148/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.4516 - accuracy: 0.5318\nEpoch 149/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.4992 - accuracy: 0.5644\nEpoch 150/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.5270 - accuracy: 0.5630\nEpoch 151/170\n15/15 [==============================] - 0s 3ms/step - loss: 1.2589 - accuracy: 0.5645\nEpoch 152/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.2976 - accuracy: 0.5333\nEpoch 153/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.9027 - accuracy: 0.5356\nEpoch 154/170\n15/15 [==============================] - 0s 3ms/step - loss: 1.9690 - accuracy: 0.4458\nEpoch 155/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.9737 - accuracy: 0.5312\nEpoch 156/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.6019 - accuracy: 0.3988\nEpoch 157/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.2023 - accuracy: 0.5605\nEpoch 158/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.4075 - accuracy: 0.5549\nEpoch 159/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.6029 - accuracy: 0.5437\nEpoch 160/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.6680 - accuracy: 0.4980\nEpoch 161/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.4049 - accuracy: 0.4417\nEpoch 162/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.0531 - accuracy: 0.5233\nEpoch 163/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.9590 - accuracy: 0.5373\nEpoch 164/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.7738 - accuracy: 0.3561\nEpoch 165/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.9683 - accuracy: 0.4497\nEpoch 166/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.4707 - accuracy: 0.5963\nEpoch 167/170\n15/15 [==============================] - 0s 2ms/step - loss: 2.7971 - accuracy: 0.4567\nEpoch 168/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.7311 - accuracy: 0.4260\nEpoch 169/170\n15/15 [==============================] - 0s 1ms/step - loss: 1.7083 - accuracy: 0.3989\nEpoch 170/170\n15/15 [==============================] - 0s 2ms/step - loss: 1.5609 - accuracy: 0.4741\nDone\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-047a5571-4c4c-459e-8813-a3f719fc2918",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2b0d0c48",
    "execution_start": 1624722977711,
    "execution_millis": 392,
    "deepnote_cell_type": "code"
   },
   "source": "nltk.download('wordnet')",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 4,
     "data": {
      "text/plain": "True"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=e9d3249a-1a9f-4785-bc7e-9323579d4b64' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "10b5df9e-ad16-4985-a60c-ad7b5a0ea370",
  "deepnote_execution_queue": []
 }
}
